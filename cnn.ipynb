{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS524 NLP - Project 2: \n",
    "# Binary Authorship Attribution G.K. Chesterton using BERT\n",
    "# Team 7: Zack Malkmus, Tyler Nitzsche, Andrew Meuller, Gabriel Laboy\n",
    "#\n",
    "# TO RUN:\n",
    "#   1. Install jupyter notebooks\n",
    "#   2. 'pip install -r requirements.txt'\n",
    "#   3. Run the code in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Import Libraries\n",
    "# --------------------------------------------\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries:\n",
      "                                                text  label\n",
      "0  \\n“But why Turkish?” asked Mr. Sherlock Holmes...      0\n",
      "1  \\nOf all the problems which have been submitte...      0\n",
      "2  Valentin, Chief of the Paris Police, was late ...      1\n",
      "3  \\nOn glancing over my notes of the seventy odd...      0\n",
      "4  \\n    \"Monsieur Arsène Lupin has the honour to...      0\n",
      "\n",
      "Null values in each column:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Reading Dataset\n",
    "# --------------------------------------------\n",
    "\n",
    "df = pd.read_csv('text_to_authorship.csv')\n",
    "\n",
    "print(\"First 5 entries:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNull values in each column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Data Preprocessing\n",
    "# --------------------------------------------\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Dataset and DataLoaders\n",
    "# --------------------------------------------\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=False,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "            \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNTextClassifier(\n",
       "  (embedding): Embedding(30522, 128)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Model Setup\n",
    "# --------------------------------------------\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes, kernel_sizes, num_filters, dropout=0.5):\n",
    "        super(CNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (K, embed_size)) for K in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        x = [nn.functional.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 128\n",
    "num_classes = 2\n",
    "kernel_sizes = [3, 4, 5]\n",
    "num_filters = 100\n",
    "dropout = 0.5\n",
    "\n",
    "model = CNNTextClassifier(vocab_size, embed_size, num_classes, kernel_sizes, num_filters, dropout)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training loss: 0.6560\n",
      "Epoch 2/3\n",
      "Training loss: 0.4059\n",
      "Epoch 3/3\n",
      "Training loss: 0.1679\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "            \n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Training loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9756\n",
      "Classification Report:\n",
      "{'Other Authors': {'precision': 0.96, 'recall': 1.0, 'f1-score': 0.9795918367346939, 'support': 24.0}, 'G.K. Chesterton': {'precision': 1.0, 'recall': 0.9411764705882353, 'f1-score': 0.9696969696969697, 'support': 17.0}, 'accuracy': 0.975609756097561, 'macro avg': {'precision': 0.98, 'recall': 0.9705882352941176, 'f1-score': 0.9746444032158318, 'support': 41.0}, 'weighted avg': {'precision': 0.9765853658536585, 'recall': 0.975609756097561, 'f1-score': 0.9754890869873447, 'support': 41.0}}\n",
      "Confusion Matrix:\n",
      "[[24  0]\n",
      " [ 1 16]]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Evaluation\n",
    "# --------------------------------------------\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "                \n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions, target_names=['Other Authors', 'G.K. Chesterton'], output_dict=True)\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    return accuracy, report, cm\n",
    "\n",
    "accuracy, report, cm = eval_model(model, val_loader, device)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Save Evaluation Results\n",
    "# --------------------------------------------\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "csv_header = [\n",
    "    'Name',\n",
    "    'Accuracy',\n",
    "    'Precision_Other Authors', 'Recall_Other Authors', 'F1-Score_Other Authors', 'Support_Other Authors',\n",
    "    'Precision_G.K. Chesterton', 'Recall_G.K. Chesterton', 'F1-Score_G.K. Chesterton', 'Support_G.K. Chesterton',\n",
    "    'Confusion_Matrix_TN', 'Confusion_Matrix_FP', 'Confusion_Matrix_FN', 'Confusion_Matrix_TP'\n",
    "]\n",
    "\n",
    "name = 'CNN'\n",
    "\n",
    "csv_row = [\n",
    "    name,\n",
    "    accuracy,\n",
    "    report['Other Authors']['precision'], report['Other Authors']['recall'], report['Other Authors']['f1-score'], report['Other Authors']['support'],\n",
    "    report['G.K. Chesterton']['precision'], report['G.K. Chesterton']['recall'], report['G.K. Chesterton']['f1-score'], report['G.K. Chesterton']['support'],\n",
    "    *cm_flat\n",
    "]\n",
    "\n",
    "csv_file = 'evaluation_results.csv'\n",
    "file_exists = os.path.isfile(csv_file)\n",
    "\n",
    "with open(csv_file, mode='a', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    if not file_exists:\n",
    "        writer.writerow(csv_header)\n",
    "    writer.writerow(csv_row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
