{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CS524 NLP - Project 2: \n",
    "# Binary Authorship Attribution G.K. Chesterton using BERT\n",
    "# Team 7: Zack Malkmus, Tyler Nitzsche, Andrew Meuller, Gabriel Laboy\n",
    "#\n",
    "# TO RUN:\n",
    "#   1. Install jupyter notebooks\n",
    "#   2. 'pip install -r requirements.txt'\n",
    "#   3. Run the code in the jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zmalk/conda/envs/nlp/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Import Libraries\n",
    "# --------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries:\n",
      "                                                text  label\n",
      "0  \\n“But why Turkish?” asked Mr. Sherlock Holmes...      0\n",
      "1  \\nOf all the problems which have been submitte...      0\n",
      "2  Valentin, Chief of the Paris Police, was late ...      1\n",
      "3  \\nOn glancing over my notes of the seventy odd...      0\n",
      "4  \\n    \"Monsieur Arsène Lupin has the honour to...      0\n",
      "\n",
      "Null values in each column:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Reading Dataset\n",
    "# --------------------------------------------\n",
    "\n",
    "df = pd.read_csv('text_to_authorship.csv')\n",
    "\n",
    "print(\"First 5 entries:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nNull values in each column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Data Preprocessing\n",
    "# --------------------------------------------\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['text'],\n",
    "    df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Dataset and DataLoaders\n",
    "# --------------------------------------------\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=False,  # We don't need attention masks\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "            \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNTextClassifier(\n",
       "  (embedding): Embedding(30522, 128)\n",
       "  (convs): ModuleList(\n",
       "    (0): Conv2d(1, 100, kernel_size=(3, 128), stride=(1, 1))\n",
       "    (1): Conv2d(1, 100, kernel_size=(4, 128), stride=(1, 1))\n",
       "    (2): Conv2d(1, 100, kernel_size=(5, 128), stride=(1, 1))\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=300, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Model Setup\n",
    "# --------------------------------------------\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_classes, kernel_sizes, num_filters, dropout=0.5):\n",
    "        super(CNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (K, embed_size)) for K in kernel_sizes\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_size)\n",
    "        x = x.unsqueeze(1)     # (batch_size, 1, seq_len, embed_size)\n",
    "        x = [nn.functional.relu(conv(x)).squeeze(3) for conv in self.convs]  # [(batch_size, num_filters, seq_len - K + 1)]\n",
    "        x = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in x]   # [(batch_size, num_filters)]\n",
    "        x = torch.cat(x, 1)    # (batch_size, num_filters * len(kernel_sizes))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "# Parameters\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embed_size = 128\n",
    "num_classes = 2\n",
    "kernel_sizes = [3, 4, 5]\n",
    "num_filters = 100\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model\n",
    "model = CNNTextClassifier(vocab_size, embed_size, num_classes, kernel_sizes, num_filters, dropout)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Training loss: 0.7545\n",
      "Epoch 2/3\n",
      "Training loss: 0.2893\n",
      "Epoch 3/3\n",
      "Training loss: 0.1764\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Training\n",
    "# --------------------------------------------\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "            \n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    print(f'Training loss: {train_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9756\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Other Authors       0.96      1.00      0.98        24\n",
      "G.K. Chesterton       1.00      0.94      0.97        17\n",
      "\n",
      "       accuracy                           0.98        41\n",
      "      macro avg       0.98      0.97      0.97        41\n",
      "   weighted avg       0.98      0.98      0.98        41\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24  0]\n",
      " [ 1 16]]\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------\n",
    "# Evaluation\n",
    "# --------------------------------------------\n",
    "\n",
    "def eval_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "                \n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    report = classification_report(true_labels, predictions, target_names=['Other Authors', 'G.K. Chesterton'])\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    return accuracy, report, cm\n",
    "\n",
    "accuracy, report, cm = eval_model(model, val_loader, device)\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "print('Confusion Matrix:')\n",
    "print(cm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
